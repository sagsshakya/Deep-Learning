{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Learning Intro."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- teaches a computer to filter inputs through layers to learn how to predict and classify information. - Observations can be in the form of images, text, or sound.\n",
    "- attempts to mimic the activity in layers of neurons in the neocortex.\n",
    "- You get input from observation and you put your input into one layer. That layer creates an output which in turn becomes the input for the next layer, and so on. This happens over and over until your final output signal!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The input information is broken down into numbers and the bits of binary data that a computer can use. (We need to either standardize or normalize these variables so that they’re within the same range.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What about synapses? Each of the synapses gets assigned weights.\n",
    "-  By adjusting the weights, the ANN decides to what extent signals get passed along.\n",
    "- When you’re training your network, you’re deciding how the weights are adjusted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do ANNs run?\n",
    "- In neural networks, you tell your network the inputs and what you want for the outputs, and let it learn on its own.\n",
    "- We can avoid the necessity of entering in all the rules. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The information goes back, and the neural network begins to learn with the goal of minimizing the cost function by tweaking the weights. This process is called backpropagation.\n",
    "##### In forward propagation, information is entered into the input layer and propagates forward through the network to get our output values. We compare the values to our expected results. Next, we calculate the errors and propagate the info backward. \n",
    "- Backpropagation allows us to adjust all the weights simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a weighted sum?\n",
    "-Inputs to a neuron can either be features from a training set or outputs from the neurons of a previous layer.\n",
    "- Each connection between two neurons has a unique synapse with a unique weight attached.\n",
    "- If you want to get from one neuron to the next, you have to travel along the synapse and pay the “toll” (weight).\n",
    "- The neuron then applies an <b><mark>activation function</mark></b> to the sum of the weighted inputs from each incoming synapse. It passes the result on to all the neurons in the next layer.\n",
    "- When we talk about updating weights in a network, we’re talking about adjusting the weights on these synapses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A neuron’s input is the sum of weighted outputs from all the neurons in the previous layer.\n",
    "- Each input is multiplied by the weight associated with the synapse connecting the input to the current neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is an activation function?\n",
    "\n",
    "- <b>The activation function of a node defines the output of that node.\n",
    "</b>\n",
    "- translates the input signals to output signals.\n",
    "- maps the output values on a range like 0 to 1 or -1 to 1.\n",
    "- a number that represents the likelihood that the cell will fire. At it’s simplest, the function is binary: yes (the neuron fires) or no (the neuron doesn’t fire).\n",
    "- If you were using a function that maps a range between 0 and 1 to determine the likelihood that an image is a cat, for example, an output of 0.9 would show a 90% probability that your image is, in fact, a cat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Four Common types of Activation Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Threshold function:\n",
    "- This is a step function. If the summed value of the input reaches a certain threshold the function passes on 0. If it’s equal to or more than zero, then it would pass on 1.\n",
    "- It’s a very rigid, straightforward, yes or no function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid function:\n",
    "- This function is used in logistic regression.\n",
    "- Unlike the threshold function, it’s a smooth, gradual progression from 0 to 1.\n",
    "- It’s very useful in the output layer and is heavily used for linear regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperbolic Tangent Function:\n",
    "- Unlike the sigmoid function which goes from 0 to 1, the value goes below zero, from -1 to 1.\n",
    "- Neural networks sometimes get “stuck” during training with the sigmoid function. This happens when there’s a lot of strongly negative input that keeps the output near zero, which messes with the learning process.\n",
    "\n",
    "- Example hyperbolic tangent function (tanh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rectifier function:\n",
    "- It’s the most efficient and biologically plausible. \n",
    "- Even though it has a kink, it’s smooth and gradual after the kink at 0. This means, for example, that your output would be either “no” or a percentage of “yes.”\n",
    "- This function doesn’t require normalization or other complicated calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The End."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
